{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Alternative\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Alternative\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3102 - acc: 0.9141\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.1445 - acc: 0.9580\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1030 - acc: 0.9701\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0802 - acc: 0.9764\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0644 - acc: 0.9809\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0526 - acc: 0.9843\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0442 - acc: 0.9865\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0359 - acc: 0.9896\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0309 - acc: 0.9911\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0265 - acc: 0.9925\n",
      "Test score: 0.0784792294316\n",
      "Test accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "## PROBLEM C\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "\n",
    "## Importing the MNIST dataset using Keras\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "## Transform each input data point into a single vector\n",
    "X_new_train = X_train.reshape(60000, 784) / 255\n",
    "X_new_test = X_test.reshape(10000, 784) / 255\n",
    "  \n",
    "## Transform the labels into a one hot vector using np_utils.to_categorical\n",
    "from keras.utils import np_utils\n",
    "Y_new_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_new_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "## Create your own model here given the constraints in the problem\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()\n",
    "\n",
    "## Implement the model and specify the parameters\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_new_train, Y_new_train, batch_size=60, epochs=10,\n",
    "    verbose=1)\n",
    "\n",
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(X_new_test, Y_new_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_new_train))\n",
    "print(np.shape(Y_new_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_143 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.3537 - acc: 0.8941\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 19s 313us/step - loss: 0.1655 - acc: 0.9500\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 18s 296us/step - loss: 0.1281 - acc: 0.9613\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.1102 - acc: 0.9652\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0950 - acc: 0.9698\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0863 - acc: 0.9723\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0806 - acc: 0.9746\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0739 - acc: 0.9763\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0692 - acc: 0.9781\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.0661 - acc: 0.9779\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.0611 - acc: 0.9800\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 19s 312us/step - loss: 0.0605 - acc: 0.9800\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 17s 275us/step - loss: 0.0542 - acc: 0.9815\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0544 - acc: 0.9818\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0518 - acc: 0.9829\n",
      "Test score: 0.0719292155433\n",
      "Test accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "## PROBLEM D\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "\n",
    "## Importing the MNIST dataset using Keras\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "## Transform each input data point into a single vector\n",
    "X_new_train = X_train.reshape(60000, 784) / 255\n",
    "X_new_test = X_test.reshape(10000, 784) / 255\n",
    "  \n",
    "## Transform the labels into a one hot vector using np_utils.to_categorical\n",
    "from keras.utils import np_utils\n",
    "Y_new_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_new_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "## Create your own model here given the constraints in the problem\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()\n",
    "\n",
    "## Implement the model and specify the parameters\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_new_train, Y_new_train, batch_size=60, epochs=15,\n",
    "    verbose=1)\n",
    "\n",
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(X_new_test, Y_new_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_136 (Dense)            (None, 800)               628000    \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 100)               80100     \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 719,210\n",
      "Trainable params: 719,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 22s 363us/step - loss: 0.6628 - acc: 0.7961\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.2268 - acc: 0.9335\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.1573 - acc: 0.9539\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.1207 - acc: 0.9640\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0979 - acc: 0.9704\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0830 - acc: 0.9749\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0696 - acc: 0.9789\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 17s 277us/step - loss: 0.0577 - acc: 0.9822\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 0.0522 - acc: 0.9840\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0457 - acc: 0.9857\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0422 - acc: 0.9871\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.0346 - acc: 0.9892\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0312 - acc: 0.9903\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.0284 - acc: 0.9915\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.0269 - acc: 0.9915\n",
      "Test score: 0.0618967362291\n",
      "Test accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "\n",
    "## Importing the MNIST dataset using Keras\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "## Transform each input data point into a single vector\n",
    "X_new_train = X_train.reshape(60000, 784) / 255\n",
    "X_new_test = X_test.reshape(10000, 784) / 255\n",
    "  \n",
    "## Transform the labels into a one hot vector using np_utils.to_categorical\n",
    "from keras.utils import np_utils\n",
    "Y_new_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_new_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "## Create your own model here given the constraints in the problem\n",
    "model = Sequential()\n",
    "model.add(Dense(800, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()\n",
    "\n",
    "## Implement the model and specify the parameters\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_new_train, Y_new_train, batch_size=1000, epochs=15,\n",
    "    verbose=1)\n",
    "\n",
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(X_new_test, Y_new_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
